{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JUST A NOTEBOOK FOR TRYING OUT STUFF, MAKING EDITS TO CODE AND ALSO USED FOR FINDING BINS FOR BINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np ,json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(r\"C:\\Users\\asus\\Desktop\\masters\\SEM 3\\alphanli\\tsv\\train.tsv\",sep='\\t',on_bad_lines='skip',header=None)\n",
    "dev=pd.read_csv(r\"C:\\Users\\asus\\Desktop\\masters\\SEM 3\\alphanli\\tsv\\dev.tsv\",sep='\\t',on_bad_lines='skip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file='train.jsonl'\n",
    "labels_file='train-labels.lst'\n",
    "train_file=\"train.json\"\n",
    "train_ratio=0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "with open(input_file, 'r') as file:\n",
    "    data=[line.strip() for line in file.readlines()]\n",
    "with open(labels_file, 'r') as file:\n",
    "    labels=[line.strip() for line in file.readlines()]\n",
    "\n",
    "#Error if length doesnt match\n",
    "if len(data)!=len(labels):\n",
    "    raise ValueError(f\"Number of data items ({len(data)}) does not match number of labels ({len(labels)})\")\n",
    "\n",
    "# Making use of indices to ensure same label and data match\n",
    "indices=list(range(len(labels)))\n",
    "\n",
    "# Shuffle indices\n",
    "random.shuffle(indices)\n",
    "\n",
    "#split shuffled indices based on train ratio\n",
    "split_idx=int(len(indices)*train_ratio)\n",
    "\n",
    "# set train and test indices\n",
    "train_indices=indices[:split_idx]\n",
    "test_indices=indices[split_idx:]\n",
    "\n",
    "# split data and labels based on indices\n",
    "train_data=[data[i] for i in train_indices]\n",
    "test_data=[data[i] for i in test_indices]\n",
    "\n",
    "train_labels=[labels[i] for i in train_indices]\n",
    "test_labels=[labels[i] for i in test_indices]\n",
    "\n",
    "# Write training and testing data and labels to file\n",
    "with open(train_file, 'w') as f:\n",
    "    json.dumps(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(preds, trues):\n",
    "    tp = sum((p == 1 and t == 1) for p, t in zip(preds, trues))\n",
    "    tn = sum((p == 0 and t == 0) for p, t in zip(preds, trues))\n",
    "    fp = sum((p == 1 and t == 0) for p, t in zip(preds, trues))\n",
    "    fn = sum((p == 0 and t == 1) for p, t in zip(preds, trues))\n",
    "\n",
    "    precision= tp/tp+fp\n",
    "    recall= tp/tp+fn\n",
    "    accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "    f1 = 2*precision*recall/(precision+recall)\n",
    "    print(f\"Accuracy={accuracy}, F1 Score={f1}\")\n",
    "    return tp, tn, fp, fn, f1, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds=train_df.iloc[:, -1].tolist()\n",
    "evaluate_predictions(preds,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = list(map(int, input(\"predictions: \").split()))\n",
    "trues = list(map(int, input(\"true labels: \").split()))\n",
    "with open(actual) as act:\n",
    "    trues=[line.strip()  for line in act.readlines()]\n",
    "with open(pred ) as pred:\n",
    "    preds=[line.strip() for line in pred.readlines()]\n",
    "tp, tn, fp, fn, f1 = evaluate_predictions(preds, trues)\n",
    "\n",
    "print(f\"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json')as f:\n",
    "    train=[i.strip() for i in f.readlines()]\n",
    "with open('train_labels.lst') as f:\n",
    "    train_labels=[i.strip() for i in f.readlines()]\n",
    "\n",
    "with open('dev.jsonl')as f:\n",
    "    dev=f.readlines()\n",
    "with open('dev-labels.lst') as f:\n",
    "    dev_labels=f.readlines()\n",
    "with open('test.json')as f:\n",
    "    test=[i.strip() for i in f.readlines()]\n",
    "with open('test_labels.lst') as f:\n",
    "    test_labels=[i.strip() for i in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "input=\"test.json\"\n",
    "with open(input) as f:\n",
    "        json.loads(input)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace(\"\\n\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"train.json\")as file:\n",
    "    data=file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d=pd.DataFrame(datap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_labels.lst') as labels:\n",
    "    y=labels.readlines()\n",
    "y=[i.strip() for i in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['Y']=y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1=d[['story_id', 'obs1', 'obs2', 'hyp1', 'Y']].rename(columns={'hyp1','hyp'})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=list(d.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.melt(id_vars=cols,value_vars=['hyp1','hyp2'], value_name='hyp',var_name='hyp_or')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "datap=[]\n",
    "for i in data:\n",
    "    datap.append(json.loads(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow_vectors_optimized(df, vocab):\n",
    "    vocab = set(vocab)  # convert to set for fast lookup\n",
    "    word_to_index = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Pre-allocate result list of dictionaries (sparse vectors)\n",
    "    X = [{} for _ in range(len(df))]\n",
    "\n",
    "    # Each text segment contributes with a different offset\n",
    "    segments = ['cleaned_text', 'hyp', 'obs1', 'obs2']\n",
    "    \n",
    "    for segment_num, segment in enumerate(segments):\n",
    "        offset = segment_num * vocab_size\n",
    "\n",
    "        for i, text in enumerate(df[segment]):\n",
    "            words = text.lower().split()\n",
    "            counts = Counter(w for w in words if w in vocab)\n",
    "            for word, count in counts.items():\n",
    "                X[i][word_to_index[word] + offset] = count\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from read_data import read_data\n",
    "from preprocess import remove_punctuations\n",
    "train_df=read_data('train.json','train_labels.lst')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "983 μs ± 25.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "from evaluation import evaluate_predictions\n",
    "Y= train_df['Y']\n",
    "%timeit x=evaluate_predictions(Y,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13 ms ± 64.3 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate_predictions2(preds, trues):\n",
    "    preds = np.array(preds)\n",
    "    trues = np.array(trues)\n",
    "    return np.mean(preds == trues)\n",
    "%timeit x=evaluate_predictions2(Y,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.iloc[0]['obs1']==train_df.iloc[1000]['obs1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import create_vocabulary\n",
    "stopwords=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','a','an','the']\n",
    "punc=''.join(['.',',',\"'\",\"'\",\":\",\"?\",'!','@','/','&']) \n",
    "vocab=create_vocabulary(train_df,stopwords,punc)\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit X=generate_bow_vectors(train_df,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(df, stopwords, punc):\n",
    "    full_text = ' '.join(df[['cleaned_text']].astype(str))\n",
    "    full_text = full_text.translate(str.maketrans('', '', ''.join(punc)))  # Faster punctuation removal\n",
    "    words = full_text.lower()\n",
    "    words=words.split()\n",
    "    vocab = set(word for word in words if word not in stopwords)\n",
    "    return sorted(vocab)\n",
    "def create_vocabulary2(df, stopwords, punc):\n",
    "    full_text = ' '.join(df[['cleaned_text']].astype(str))\n",
    "    full_text = full_text.translate(str.maketrans('', '', ''.join(punc)))  # Faster punctuation removal\n",
    "    words = full_text.lower()\n",
    "    words=words.split()\n",
    "    words=set(words)\n",
    "    vocab = set(word for word in words if  word not in stopwords)\n",
    "    return sorted(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit vocab=create_vocabulary(train_df,stopwords,punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.2 ms ± 772 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit vocab2=create_vocabulary2(train_df,stopwords,punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle to store and load model\n",
    "import _pickle as pickle\n",
    "def store_model(model, filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    return\n",
    "def load_model(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        model=pickle.load(f)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def step_fn(x):\n",
    "    return np.where(x > 0, 1, -1)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, l_r=0.02, n_iters=10000):\n",
    "        self.lr = l_r\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_fn = step_fn\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.best_weights=None\n",
    "        self.max_acc=0\n",
    "\n",
    "    def partial_fit(self, X_train, Y, X_dev,Y_dev):\n",
    "        '''\n",
    "        Inputs: \n",
    "          For training:\n",
    "            X_train         list of features represented in the form of a sparse vector dictionary for each row of training data\n",
    "            Y               list of actual predictions for the training data\n",
    "          To check change in accuracy and save best weights\n",
    "            X_dev           list of features represented in the form of a sparse vector dictionary for each row of dev data\n",
    "            Y_dev           list of actual predictions for the dev data\n",
    "\n",
    "        Outputs:\n",
    "            Prints accuracy for each iteration\n",
    "            saves latest weights and bias and best weights for model  \n",
    "        '''\n",
    "        n_samples = len(Y)\n",
    "        n_features = len(vocab)*4 + num_of_additional_features\n",
    "        self.weights = np.random.rand(n_features) if self.weights is None else self.weights\n",
    "        self.bias = 0 if self.bias is None else self.bias\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "              \n",
    "            for x_dict, y_true in zip(X_train, Y):\n",
    "                score=0 # initialize value to 0 for each row of training data\n",
    "                for index, value in x_dict.items():\n",
    "                    score+= self.weights[index]*value\n",
    "                y_pred = self.activation_fn(score+self.bias)\n",
    "                if y_pred != y_true:\n",
    "                    update = self.lr * (y_true - y_pred)\n",
    "                    for index, value in x_dict.items():\n",
    "                        self.weights[index]+= update * value \n",
    "                    self.bias += update\n",
    "\n",
    "            y_pred=[self.pred(x) for x in X_dev]\n",
    "            accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "            if accuracy>self.max_acc:\n",
    "                print (f'iter {i+1} of {self.n_iters} : Accuracy = {accuracy}')\n",
    "                self.best_weights=self.weights\n",
    "                self.max_acc=accuracy\n",
    "\n",
    "    def pred(self, X,best=False):\n",
    "        weights=self.weights\n",
    "        if best:\n",
    "            weights=self.best_weights\n",
    "        op=0\n",
    "        for i,v in X.items():\n",
    "            op+=weights[i]*v\n",
    "        y_pred = self.activation_fn(op+self.bias)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def generate_bow_vectors_optimized(df, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # Pre-allocate result list of dictionaries (sparse vectors)\n",
    "    X = [{} for _ in range(len(df))]\n",
    "\n",
    "    # Each text segment contributes with a different offset\n",
    "    segments = ['cleaned_text', 'hyp', 'obs1', 'obs2']\n",
    "    \n",
    "    for segment_num, segment in enumerate(segments):\n",
    "        offset = segment_num * vocab_size\n",
    "\n",
    "        for i, text in enumerate(df[segment]):\n",
    "            words = text.lower().split()\n",
    "            counts = Counter(w for w in words if w in vocab)\n",
    "            for word, count in counts.items():\n",
    "                X[i][word_to_index[word] + offset] = count\n",
    "\n",
    "    return X\n",
    "\n",
    "    \n",
    "            \n",
    "# Pickle to store and load model\n",
    "def store_model(model, filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    return\n",
    "def load_model(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        model=pickle.load(f)\n",
    "    return model\n",
    "'''   \n",
    "if __name__==\"__main__\":\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    from evaluation import evaluate_predictions\n",
    "    from read_data import read_data\n",
    "    from preprocess import remove_punctuations,create_vocabulary\n",
    "\n",
    "    batch_size=2000\n",
    "\n",
    "    train_data='train.json'\n",
    "    train_labels='train_labels.lst'\n",
    "\n",
    "    test_data=\"test.json\"\n",
    "    test_labels='test_labels.lst'\n",
    "    dev_data='dev.jsonl'\n",
    "    dev_labels='dev-labels.lst'\n",
    "    stopwords=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','a','an','the']\n",
    "    punc=''.join(['.',',',\"'\",\"'\",\":\",\"?\",'!','@','/','&'])\n",
    "    #read train data\n",
    "    train_df=read_data(train_data,train_labels)\n",
    "    #print(type(train_df))\n",
    "    vocab=create_vocabulary(train_df,stopwords,punc)\n",
    "    #print(vocab)\n",
    "    vocab_size=len(vocab)\n",
    "    word_to_index = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "    X_gen = generate_bow_vectors_optimized(train_df, vocab)\n",
    "    #print(X_gen[0]==X_gen[1002])\n",
    "    # X_additional=\n",
    "    additional_features=[]\n",
    "    num_of_additional_features=len(additional_features)\n",
    "    Y = train_df['Y'].values \n",
    "    dev_df=read_data(dev_data,dev_labels)\n",
    "    X_dev=generate_bow_vectors_optimized(dev_df,vocab)\n",
    "    Y_dev=dev_df['Y'].values\n",
    "    # training\n",
    "    model = Perceptron()\n",
    "    # model=load_model('model1.pkl')\n",
    "    print(\"Training Model\")\n",
    "    model.partial_fit(X_gen, Y, X_dev,Y_dev)\n",
    "    model_fname=\"model2.pkl\"\n",
    "    print(f\"Storing Model {model_fname}\")\n",
    "    store_model(model,model_fname)\n",
    "\n",
    "    \n",
    "\n",
    "    # testing model performance using dev data\n",
    "    \n",
    "    y_pred=[model.pred(x) for x in X_dev]\n",
    "    accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "\n",
    "    # Accuracy=0.5094647519582245\n",
    "\n",
    "'''\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def step_fn(x):\n",
    "    return np.where(x > 0, 1, -1)\n",
    "\n",
    "class Perceptron2:\n",
    "    def __init__(self, l_r=0.02, n_iters=10000,best=False):\n",
    "        self.lr = l_r\n",
    "        self.n_iters = n_iters\n",
    "        self.activation_fn = step_fn\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.best_weights=None\n",
    "        self.best_bias=None\n",
    "        self.max_acc=0\n",
    "        self.best=best\n",
    "\n",
    "    def partial_fit(self, X_train, Y, X_dev,Y_dev):\n",
    "        '''\n",
    "        Inputs: \n",
    "          For training:\n",
    "            X_train         list of features represented in the form of a sparse vector dictionary for each row of training data\n",
    "            Y               list of actual predictions for the training data\n",
    "          To check change in accuracy and save best weights\n",
    "            X_dev           list of features represented in the form of a sparse vector dictionary for each row of dev data\n",
    "            Y_dev           list of actual predictions for the dev data\n",
    "\n",
    "        Outputs:\n",
    "            Prints accuracy for each iteration\n",
    "            saves latest weights and bias and best weights for model  \n",
    "        '''\n",
    "        n_samples = len(Y)\n",
    "        n_features = len(vocab)*4 + num_of_additional_features\n",
    "        self.weights = np.random.rand(n_features) if self.weights is None else self.weights\n",
    "        self.bias = 0 if self.bias is None else self.bias\n",
    "\n",
    "        for i in range(self.n_iters):\n",
    "              \n",
    "            for x_list, y_true in zip(X_train, Y):\n",
    "                score=0 # initialize value to 0 for each row of training data\n",
    "                for index, value in x_list:\n",
    "                    score+= self.weights[index]*value\n",
    "                y_pred = self.activation_fn(score+self.bias)\n",
    "                if y_pred != y_true:\n",
    "                    update = self.lr * (y_true - y_pred)\n",
    "                    for index, value in x_list:\n",
    "                        self.weights[index]+= update * value \n",
    "                    self.bias += update\n",
    "\n",
    "            y_pred=[self.pred(x, self.best)for x in X_dev]\n",
    "            accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "            if accuracy>self.max_acc:\n",
    "                print (f'iter {i+1} of {self.n_iters} : Accuracy = {accuracy}')\n",
    "                self.best_weights=self.weights\n",
    "                self.best_bias=self.bias\n",
    "                self.max_acc=accuracy\n",
    "\n",
    "    def pred(self, X,best=False):\n",
    "        weights=self.weights\n",
    "        bias=self.bias\n",
    "        if best:\n",
    "            weights=self.best_weights\n",
    "            bias=self.best_bias\n",
    "        op=0\n",
    "        for i,v in X:\n",
    "            op+=weights[i]*v\n",
    "        y_pred = self.activation_fn(op+bias)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "\n",
    "def generate_bow_vectors_optimized2(df, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    X = [[]for i in range(len(df))]\n",
    "    # Each text segment contributes with a different offset\n",
    "    segments = ['cleaned_text', 'hyp', 'obs1', 'obs2']\n",
    "    \n",
    "    for segment_num, segment in enumerate(segments):\n",
    "        offset = segment_num * vocab_size\n",
    "        \n",
    "        for i, text in enumerate(df[segment]):\n",
    "            \n",
    "            words = text.lower().split()\n",
    "            counts = Counter(w for w in words if w in vocab)\n",
    "            for word, count in counts.items():\n",
    "                idx = word_to_index[word] + offset\n",
    "                X[i].append((idx,count))\n",
    "            extract_features(text)\n",
    "    return X\n",
    "\n",
    "    \n",
    "            \n",
    "# Pickle to store and load model\n",
    "def store_model(model, filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    return\n",
    "def load_model(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        model=pickle.load(f)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7.84 s\n",
      "Wall time: 13.1 s\n"
     ]
    }
   ],
   "source": [
    "word_to_index = {word: i for i, word in enumerate(sorted(vocab))}\n",
    "%time X_gen2 = generate_bow_vectors_optimized2(train_df, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[]\n",
    "x.append((1,2))\n",
    "x.append((1,2))\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3064\n",
      "3064\n",
      "3064\n",
      "3064\n",
      "3064 3064\n"
     ]
    }
   ],
   "source": [
    "dev_df=read_data('dev.jsonl','dev-labels.lst')\n",
    "X_dev2=generate_bow_vectors_optimized2(dev_df,vocab)\n",
    "Y_dev=dev_df['Y']\n",
    "print(len(X_dev2),len(Y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model('model2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 1 : Accuracy = 0.5003263707571801\n",
      "iter 1 of 1 : Accuracy = 0.5048955613577023\n",
      "iter 1 of 1 : Accuracy = 0.5130548302872062\n",
      "5.69 s ± 152 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model=Perceptron(n_iters=1)\n",
    "Y=train_df['Y']\n",
    "\n",
    "num_of_additional_features=0\n",
    "\n",
    "%timeit model.partial_fit(X_gen, Y ,X_dev ,dev_df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gen2=generate_bow_vectors_optimized(train_df,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 1 : Accuracy = 0.5\n",
      "3.49 s ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "model2=Perceptron2(n_iters=1)\n",
    "Y=train_df['Y']\n",
    "\n",
    "num_of_additional_features=0\n",
    "\n",
    "%timeit model2.partial_fit(X_gen2, Y ,X_dev2 ,dev_df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.41 μs ± 101 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x=X_dev[1]\n",
    "%timeit sum(model.weights[idx] * val for idx, val in x.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.6 ms ± 583 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit y_pred = [model.pred(x) for x in X_dev]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_batch_pred(X, weights, bias, activation_fn):\n",
    "   def fast_batch_pred(X, weights, bias, activation_fn):\n",
    "    results = []\n",
    "    for x in X:\n",
    "        op = sum(weights[idx] * val for idx, val in x.items())\n",
    "        results.append(activation_fn(op + bias))\n",
    "    return results\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.1 ms ± 388 μs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit fast_batch_pred(X_dev,model.weights,model.bias,model.activation_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(x):\n",
    "    op=0\n",
    "    for i,v in x.items():\n",
    "        op+=model.weights[i]*v\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.35 μs ± 55.3 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit run(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/10: Accuracy = 0.5000, Max Accuracy = 0.0000\n",
      "Iter 2/10: Accuracy = 0.5134, Max Accuracy = 0.5000\n",
      "CPU times: total: 3min 34s\n",
      "Wall time: 3min 55s\n"
     ]
    }
   ],
   "source": [
    "model2=Perceptron2(n_iters=10)\n",
    "%time model2.partial_fit(X_train2, Y_train, X_dev2, dev_df['Y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "\n",
    "NEGATIONS = {\"no\", \"not\", \"never\", \"none\", \"nothing\", \"neither\", \"nobody\", \"nowhere\"}\n",
    "POSITIVE_WORDS = {\"good\", \"happy\", \"love\", \"excellent\", \"great\", \"positive\", \"joy\"}\n",
    "NEGATIVE_WORDS = {\"bad\", \"sad\", \"hate\", \"terrible\", \"awful\", \"negative\", \"pain\"}\n",
    "\n",
    "def tokenize(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
    "\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = set1 & set2\n",
    "    union = set1 | set2\n",
    "    return len(intersection) / len(union) if union else 0.0\n",
    "\n",
    "def joh_bins(jaccard_obs_hyp):\n",
    "    if jaccard_obs_hyp<0.06:\n",
    "        return 1,0,0,0  \n",
    "    elif jaccard_obs_hyp<0.1:\n",
    "        return 0,1,0,0\n",
    "    elif jaccard_obs_hyp<0.16:\n",
    "        return 0,0,1,0\n",
    "    else:\n",
    "        return 0,0,0,1\n",
    "\n",
    "\n",
    "def word_overlap(set1, set2):\n",
    "    return len(set1 & set2)\n",
    "\n",
    "def sentiment_score(tokens):\n",
    "    pos = len([w for w in tokens if w in POSITIVE_WORDS])\n",
    "    neg = len([w for w in tokens if w in NEGATIVE_WORDS])\n",
    "    return pos - neg\n",
    "\n",
    "def contains_negation(tokens):\n",
    "    return any(w in NEGATIONS for w in tokens)\n",
    "\n",
    "def get_overlap_bin(overlap):\n",
    "    if overlap == 1:\n",
    "        return 1,0,0,0,0,0\n",
    "    elif overlap == 2:\n",
    "        return 0,1,0,0,0,0\n",
    "    elif overlap ==3:\n",
    "        return 0,0,1,0,0,0\n",
    "    elif overlap<=5:\n",
    "        return 0,0,0,1,0,0\n",
    "    elif overlap<=10:\n",
    "        return 0,0,0,0,1,0\n",
    "    else:\n",
    "        return 0,0,0,0,0,1\n",
    "    \n",
    "\n",
    "def get_sentiment_bin(score):\n",
    "    if score < 0:\n",
    "        return 0,0,1\n",
    "    elif score == 0:\n",
    "        return 0,1,0\n",
    "    else:\n",
    "        return 1,0,0\n",
    "\n",
    "def extract_features(row):\n",
    "    obs1_tokens = tokenize(row['obs1'])\n",
    "    obs2_tokens = tokenize(row['obs2'])\n",
    "    hyp_tokens = tokenize(row['hyp'])\n",
    "    obs1_set = set(tokenize(row['obs1']))\n",
    "    obs2_set = set(tokenize(row['obs2']))\n",
    "    hyp_set = set(tokenize(row['hyp']))\n",
    "\n",
    "    all_obs_set = obs1_set | obs2_set\n",
    "\n",
    "    jaccard_obs_hyp = jaccard_similarity(all_obs_set, hyp_set)\n",
    "    joh1,joh2,joh3,joh4=joh_bins(jaccard_obs_hyp) #get bins for jaccard similarity of obs and hyp\n",
    "    jaccard_obs1_hyp = jaccard_similarity(obs1_set, hyp_set)\n",
    "    jo1h1,jo1h2,jo1h3,jo1h4=joh_bins(jaccard_obs1_hyp)#get bins for jaccard similarity of obs1 and hyp\n",
    "    jaccard_obs2_hyp = jaccard_similarity(obs2_set, hyp_set)\n",
    "    jo2h1,jo2h2,jo2h3,jo2h4=joh_bins(jaccard_obs2_hyp)#get bins for jaccard similarity of obs2 and hyp\n",
    "    \n",
    "    overlap_all = word_overlap(all_obs_set, hyp_set)\n",
    "    oa1,oa2,oa3,oa5,oa10,oa15=get_overlap_bin(overlap_all) # get bins for word overlap btw obs_all and hyp\n",
    "    overlap_obs1 = word_overlap(obs1_set, hyp_set)\n",
    "    oobs1_1,oobs1_2,oobs1_3,oobs1_5,oobs1_10,oobs1_15=get_overlap_bin(overlap_obs1) # get bins for word overlap btw obs1 and hyp\n",
    "    overlap_obs2 = word_overlap(obs2_set, hyp_set)\n",
    "    oobs2_1,oobs2_2,oobs2_3,oobs2_5,oobs2_10,oobs2_15=get_overlap_bin(overlap_obs2)# get bins for word overlap btw obs2 and hyp\n",
    "    sentiment_obs = sentiment_score(list(all_obs_set))\n",
    "    sent_obs_pos,sent_obs_neutral,sent_obs_neg=get_sentiment_bin(sentiment_obs) # get bins for sentiment of obs\n",
    "    sentiment_obs1=sentiment_score(list(obs1_tokens))\n",
    "    sent_obs1_pos,sent_obs1_neutral,sent_obs1_neg=get_sentiment_bin(sentiment_obs1)# get bins for sentiment of obs1\n",
    "    sentiment_obs2=sentiment_score(list(obs2_tokens))\n",
    "    sent_obs2_pos,sent_obs2_neutral,sent_obs2_neg=get_sentiment_bin(sentiment_obs2)# get bins for sentiment of obs2\n",
    "    sentiment_hyp = sentiment_score(list(hyp_tokens))\n",
    "    sent_hyp_pos,sent_hyp_neutral,sent_hyp_neg=get_sentiment_bin(sentiment_hyp)# get bins for sentiment of hyp\n",
    "\n",
    "    negation_obs = contains_negation(all_obs_set)\n",
    "    negation_obs1=contains_negation(obs1_set)\n",
    "    negation_obs2=contains_negation(obs2_set)\n",
    "    negation_hyp = contains_negation(hyp_set)\n",
    "\n",
    "    features = [jaccard_obs1_hyp,jo1h1,jo1h2,jo1h3,jo1h4,                       # jaccard obs1 and bins\n",
    "                jaccard_obs2_hyp,jo2h1,jo2h2,jo2h3,jo2h4,                       # jaccard obs2 and bins\n",
    "                jaccard_obs_hyp,joh1,joh2,joh3,joh4,                            # jaccard obs_all and bins\n",
    "                overlap_all,oa1,oa2,oa3,oa5,oa10,oa15,                          # overlap obs_all with hyp and bins\n",
    "                overlap_obs1,oobs1_1,oobs1_2,oobs1_3,oobs1_5,oobs1_10,oobs1_15, # overlap obs_1 with hyp and bins\n",
    "                overlap_obs2,oobs2_1,oobs2_2,oobs2_3,oobs2_5,oobs2_10,oobs2_15, # overlap obs_2 with hyp and bins\n",
    "                sentiment_obs,sent_obs_pos,sent_obs_neutral,sent_obs_neg,       # sentiment of obs_all and bins\n",
    "                sentiment_obs1,sent_obs1_pos,sent_obs1_neutral,sent_obs1_neg,   # sentiment of obs_1 and bins\n",
    "                sentiment_obs2,sent_obs2_pos,sent_obs2_neutral,sent_obs2_neg,   # sentiment of obs_2 and bins\n",
    "                sentiment_hyp,sent_hyp_pos,sent_hyp_neutral,sent_hyp_neg,       # sentiment of hyp and bins\n",
    "                int(negation_obs),int(negation_obs1),int(negation_obs2),int(negation_hyp),x:=1]# negation check of all texts\n",
    "    '''uncomment below for dictionary'''\n",
    "    # st='jaccard_obs1_hyp,jaccard_obs1_bin1,jaccard_obs1_bin2,jaccard_obs1_bin3,jaccard_obs1_bin4,' \\\n",
    "    # 'jaccard_obs2_hyp,jaccard_obs2_bin1,jaccard_obs2_bin2,jaccard_obs2_bin3,jaccard_obs2_bin4,'\n",
    "    # 'jaccard_obs_hyp,jaccard_obs_bin1,jaccard_obs_bin2,jaccard_obs_bin3,jaccard_obs_bin4,'\n",
    "    # 'overlap_all,overlap_all_bin1,overlap_all_bin2,overlap_all_bin3,overlap_all_bin4,overlap_all_bin5,overlap_all_bin6,'\n",
    "    # 'overlap_obs1,overlap_obs1_bin1,overlap_obs1_bin2,overlap_obs1_bin3,overlap_obs1_bin4,overlap_obs1_bin5,overlap_obs1_bin6,'\n",
    "    # 'overlap_obs2,overlap_obs2_bin1,overlap_obs2_bin2,overlap_obs2_bin3,overlap_obs2_bin4,overlap_obs2_bin5,overlap_obs2_bin6,'\n",
    "    # 'sentiment_obs,sentiment_obs_positive,sentiment_obs_neutral,sentiment_obs_negative,'\n",
    "    # 'sentiment_obs1,sentiment_obs1_positive,sentiment_obs1_neutral,sentiment_obs1_negative,'\n",
    "    # 'sentiment_obs2,sentiment_obs2_positive,sentiment_obs2_neutral,sentiment_obs2_negative,'\n",
    "    # 'sentiment_hyp,sentiment_hyp_positive,sentiment_hyp_neutral,sentiment_hyp_negative,'\n",
    "    # 'negation_obs,negation_obs1,negation_obs2,negation_hyp'\n",
    "    # st=st.split(',')\n",
    "    # features_dict={}\n",
    "    # for i,v in enumerate(features):\n",
    "    #     features_dict[st[i]]=v\n",
    "    \n",
    "    return features #features_dict if you want to get dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      story_id  Y  jaccard_obs1_hyp  \\\n",
      "35299   00050cbb-049e-444f-a17a-04c882da4693-1  1          0.066667   \n",
      "76179   00050cbb-049e-444f-a17a-04c882da4693-1 -1          0.272727   \n",
      "88351   00050cbb-049e-444f-a17a-04c882da4693-1  1          0.187500   \n",
      "109880  00050cbb-049e-444f-a17a-04c882da4693-1  1          0.000000   \n",
      "171022  00050cbb-049e-444f-a17a-04c882da4693-1 -1          0.333333   \n",
      "\n",
      "        jaccard_obs2_hyp  jaccard_obs_hyp  overlap_all  overlap_obs1  \\\n",
      "35299           0.062500         0.086957            2             1   \n",
      "76179           0.000000         0.166667            3             3   \n",
      "88351           0.066667         0.173913            4             3   \n",
      "109880          0.058824         0.043478            1             0   \n",
      "171022          0.181818         0.230769            3             2   \n",
      "\n",
      "        overlap_obs2  sentiment_obs  sentiment_obs1  sentiment_obs2  \\\n",
      "35299              1              0               0               0   \n",
      "76179              0              0               0               0   \n",
      "88351              1              0               0               0   \n",
      "109880             1              0               0               0   \n",
      "171022             2              0               0               0   \n",
      "\n",
      "        sentiment_hyp  negation_obs  negation_obs1  negation_obs2  \\\n",
      "35299               0             0              0              0   \n",
      "76179               0             0              0              0   \n",
      "88351               0             0              0              0   \n",
      "109880              0             1              0              1   \n",
      "171022              0             0              0              0   \n",
      "\n",
      "        negation_hyp  \n",
      "35299              0  \n",
      "76179              0  \n",
      "88351              0  \n",
      "109880             0  \n",
      "171022             0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df=train_df\n",
    "features_dict = pd.json_normalize(df.apply(extract_features, axis=1))\n",
    "df_features = pd.concat([df[['story_id', 'Y']], features_dict], axis=1)\n",
    "\n",
    "#df_features.to_excel(\"train_features.xlsx\", index=False)\n",
    "print(df_features.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['story_id', 'Y', 'jaccard_obs1_hyp', 'jaccard_obs2_hyp',\n",
       "       'jaccard_obs_hyp', 'overlap_all', 'overlap_obs1', 'overlap_obs2',\n",
       "       'sentiment_obs', 'sentiment_obs1', 'sentiment_obs2', 'sentiment_hyp',\n",
       "       'negation_obs', 'negation_obs1', 'negation_obs2', 'negation_hyp'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>jaccard_obs1_hyp</th>\n",
       "      <th>jaccard_obs2_hyp</th>\n",
       "      <th>jaccard_obs_hyp</th>\n",
       "      <th>overlap_all</th>\n",
       "      <th>overlap_obs1</th>\n",
       "      <th>overlap_obs2</th>\n",
       "      <th>sentiment_obs</th>\n",
       "      <th>sentiment_obs1</th>\n",
       "      <th>sentiment_obs2</th>\n",
       "      <th>sentiment_hyp</th>\n",
       "      <th>negation_obs</th>\n",
       "      <th>negation_obs1</th>\n",
       "      <th>negation_obs2</th>\n",
       "      <th>negation_hyp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "      <td>271446.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122569</td>\n",
       "      <td>0.092998</td>\n",
       "      <td>0.120212</td>\n",
       "      <td>2.449916</td>\n",
       "      <td>1.652332</td>\n",
       "      <td>1.398499</td>\n",
       "      <td>0.066975</td>\n",
       "      <td>0.007486</td>\n",
       "      <td>0.059798</td>\n",
       "      <td>0.013288</td>\n",
       "      <td>0.088806</td>\n",
       "      <td>0.022038</td>\n",
       "      <td>0.068257</td>\n",
       "      <td>0.075407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000002</td>\n",
       "      <td>0.093545</td>\n",
       "      <td>0.079969</td>\n",
       "      <td>0.072108</td>\n",
       "      <td>1.424343</td>\n",
       "      <td>1.183246</td>\n",
       "      <td>1.161290</td>\n",
       "      <td>0.339129</td>\n",
       "      <td>0.163099</td>\n",
       "      <td>0.300928</td>\n",
       "      <td>0.204871</td>\n",
       "      <td>0.284464</td>\n",
       "      <td>0.146806</td>\n",
       "      <td>0.252186</td>\n",
       "      <td>0.264048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Y  jaccard_obs1_hyp  jaccard_obs2_hyp  jaccard_obs_hyp  \\\n",
       "count  271446.000000     271446.000000     271446.000000    271446.000000   \n",
       "mean        0.000000          0.122569          0.092998         0.120212   \n",
       "std         1.000002          0.093545          0.079969         0.072108   \n",
       "min        -1.000000          0.000000          0.000000         0.000000   \n",
       "25%        -1.000000          0.062500          0.047619         0.066667   \n",
       "50%         0.000000          0.100000          0.076923         0.111111   \n",
       "75%         1.000000          0.166667          0.133333         0.160000   \n",
       "max         1.000000          1.000000          0.833333         0.857143   \n",
       "\n",
       "         overlap_all   overlap_obs1   overlap_obs2  sentiment_obs  \\\n",
       "count  271446.000000  271446.000000  271446.000000  271446.000000   \n",
       "mean        2.449916       1.652332       1.398499       0.066975   \n",
       "std         1.424343       1.183246       1.161290       0.339129   \n",
       "min         0.000000       0.000000       0.000000      -2.000000   \n",
       "25%         1.000000       1.000000       1.000000       0.000000   \n",
       "50%         2.000000       1.000000       1.000000       0.000000   \n",
       "75%         3.000000       2.000000       2.000000       0.000000   \n",
       "max        15.000000      12.000000      12.000000       3.000000   \n",
       "\n",
       "       sentiment_obs1  sentiment_obs2  sentiment_hyp   negation_obs  \\\n",
       "count   271446.000000   271446.000000  271446.000000  271446.000000   \n",
       "mean         0.007486        0.059798       0.013288       0.088806   \n",
       "std          0.163099        0.300928       0.204871       0.284464   \n",
       "min         -2.000000       -1.000000      -2.000000       0.000000   \n",
       "25%          0.000000        0.000000       0.000000       0.000000   \n",
       "50%          0.000000        0.000000       0.000000       0.000000   \n",
       "75%          0.000000        0.000000       0.000000       0.000000   \n",
       "max          2.000000        2.000000       3.000000       1.000000   \n",
       "\n",
       "       negation_obs1  negation_obs2   negation_hyp  \n",
       "count  271446.000000  271446.000000  271446.000000  \n",
       "mean        0.022038       0.068257       0.075407  \n",
       "std         0.146806       0.252186       0.264048  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       0.000000  \n",
       "50%         0.000000       0.000000       0.000000  \n",
       "75%         0.000000       0.000000       0.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO FIND BINS SUCH THAT DATA EVENLY DISTRIBUTED AMONG BINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jaccard_obs1_hyp\n",
      "[0.0, 0.027777777777777776, 0.029411764705882353, 0.030303030303030304, 0.03125, 0.03225806451612903, 0.03333333333333333, 0.034482758620689655, 0.03571428571428571, 0.037037037037037035, 0.038461538461538464, 0.04, 0.041666666666666664, 0.043478260869565216, 0.045454545454545456, 0.047619047619047616, 0.05, 0.05263157894736842, 0.05555555555555555, 0.058823529411764705, 0.0625, 0.06451612903225806, 0.06666666666666667, 0.06896551724137931, 0.07142857142857142, 0.07407407407407407, 0.07692307692307693, 0.08, 0.08333333333333333, 0.08571428571428572, 0.08695652173913043, 0.08823529411764706, 0.09090909090909091, 0.09375, 0.09523809523809523, 0.0967741935483871, 0.1, 0.10344827586206896, 0.10526315789473684, 0.10714285714285714, 0.1111111111111111, 0.11538461538461539, 0.11764705882352941, 0.12, 0.12121212121212122, 0.125, 0.12903225806451613, 0.13043478260869565, 0.13333333333333333, 0.13636363636363635, 0.13793103448275862, 0.14285714285714285, 0.14705882352941177, 0.14814814814814814, 0.15, 0.15151515151515152, 0.15384615384615385, 0.15625, 0.15789473684210525, 0.16, 0.16666666666666666, 0.1724137931034483, 0.17391304347826086, 0.17647058823529413, 0.17857142857142858, 0.18181818181818182, 0.18518518518518517, 0.1875, 0.19047619047619047, 0.19230769230769232, 0.2, 0.20689655172413793, 0.20833333333333334, 0.21052631578947367, 0.21428571428571427, 0.21739130434782608, 0.2222222222222222, 0.22727272727272727, 0.23076923076923078, 0.23333333333333334, 0.23529411764705882, 0.23809523809523808, 0.24, 0.25, 0.2608695652173913, 0.2631578947368421, 0.26666666666666666, 0.2727272727272727, 0.2777777777777778, 0.28, 0.2857142857142857, 0.2916666666666667, 0.29411764705882354, 0.3, 0.30434782608695654, 0.3076923076923077, 0.3125, 0.3157894736842105, 0.3181818181818182, 0.3333333333333333, 0.34782608695652173, 0.35, 0.35294117647058826, 0.35714285714285715, 0.36363636363636365, 0.3684210526315789, 0.375, 0.38095238095238093, 0.38461538461538464, 0.3888888888888889, 0.391304347826087, 0.4, 0.4117647058823529, 0.4166666666666667, 0.42105263157894735, 0.42857142857142855, 0.4375, 0.4444444444444444, 0.45, 0.45454545454545453, 0.46153846153846156, 0.4666666666666667, 0.47368421052631576, 0.5, 0.5294117647058824, 0.5333333333333333, 0.5384615384615384, 0.5454545454545454, 0.5555555555555556, 0.5625, 0.5714285714285714, 0.5833333333333334, 0.6, 0.6153846153846154, 0.625, 0.6363636363636364, 0.6428571428571429, 0.6666666666666666, 0.6875, 0.6923076923076923, 0.7, 0.7142857142857143, 0.7272727272727273, 0.7333333333333333, 0.75, 0.7692307692307693, 0.7777777777777778, 0.7857142857142857, 0.8, 0.8181818181818182, 0.8333333333333334, 0.8461538461538461, 0.8571428571428571, 0.875, 0.8888888888888888, 0.9, 1.0]\n",
      "Quartile bins (start to end):\n",
      "Q1: 0.0 to 0.0625\n",
      "Q2: 0.0625 to 0.1\n",
      "Q3: 0.1 to 0.16666666666666666\n",
      "Q4: 0.16666666666666666 to 1.0\n",
      "jaccard_obs2_hyp\n",
      "[0.0, 0.027777777777777776, 0.02857142857142857, 0.030303030303030304, 0.03125, 0.03225806451612903, 0.03333333333333333, 0.034482758620689655, 0.03571428571428571, 0.037037037037037035, 0.038461538461538464, 0.04, 0.041666666666666664, 0.043478260869565216, 0.045454545454545456, 0.047619047619047616, 0.05, 0.05263157894736842, 0.05555555555555555, 0.05714285714285714, 0.058823529411764705, 0.06060606060606061, 0.0625, 0.06451612903225806, 0.06666666666666667, 0.06896551724137931, 0.07142857142857142, 0.07407407407407407, 0.07692307692307693, 0.08, 0.08333333333333333, 0.08571428571428572, 0.08695652173913043, 0.08823529411764706, 0.09090909090909091, 0.09375, 0.09523809523809523, 0.0967741935483871, 0.1, 0.10344827586206896, 0.10526315789473684, 0.10714285714285714, 0.1111111111111111, 0.11538461538461539, 0.11764705882352941, 0.12, 0.12121212121212122, 0.125, 0.12903225806451613, 0.13043478260869565, 0.13333333333333333, 0.13636363636363635, 0.13793103448275862, 0.14285714285714285, 0.14705882352941177, 0.14814814814814814, 0.15, 0.15151515151515152, 0.15384615384615385, 0.15789473684210525, 0.16, 0.16129032258064516, 0.16666666666666666, 0.1724137931034483, 0.17391304347826086, 0.17647058823529413, 0.17857142857142858, 0.18181818181818182, 0.18518518518518517, 0.1875, 0.19047619047619047, 0.19230769230769232, 0.2, 0.20689655172413793, 0.20833333333333334, 0.21052631578947367, 0.21428571428571427, 0.21739130434782608, 0.2222222222222222, 0.22580645161290322, 0.22727272727272727, 0.23076923076923078, 0.23529411764705882, 0.23809523809523808, 0.24, 0.24242424242424243, 0.25, 0.2608695652173913, 0.2631578947368421, 0.26666666666666666, 0.2692307692307692, 0.2727272727272727, 0.2777777777777778, 0.28, 0.2857142857142857, 0.2916666666666667, 0.29411764705882354, 0.3, 0.30434782608695654, 0.3076923076923077, 0.3125, 0.3157894736842105, 0.3181818181818182, 0.3333333333333333, 0.35, 0.35294117647058826, 0.35714285714285715, 0.36363636363636365, 0.3684210526315789, 0.375, 0.38095238095238093, 0.38461538461538464, 0.3888888888888889, 0.4, 0.4117647058823529, 0.4166666666666667, 0.42105263157894735, 0.42857142857142855, 0.4375, 0.4444444444444444, 0.45, 0.45454545454545453, 0.46153846153846156, 0.4666666666666667, 0.47058823529411764, 0.5, 0.5333333333333333, 0.5384615384615384, 0.5454545454545454, 0.5555555555555556, 0.5714285714285714, 0.5833333333333334, 0.6, 0.6153846153846154, 0.625, 0.6363636363636364, 0.6666666666666666, 0.6875, 0.7, 0.7142857142857143, 0.7272727272727273, 0.75, 0.8, 0.8333333333333334]\n",
      "Quartile bins (start to end):\n",
      "Q1: 0.0 to 0.047619047619047616\n",
      "Q2: 0.047619047619047616 to 0.07692307692307693\n",
      "Q3: 0.07692307692307693 to 0.13333333333333333\n",
      "Q4: 0.13333333333333333 to 0.8333333333333334\n",
      "jaccard_obs_hyp\n",
      "[0.0, 0.024390243902439025, 0.025, 0.02564102564102564, 0.02631578947368421, 0.02702702702702703, 0.027777777777777776, 0.02857142857142857, 0.029411764705882353, 0.030303030303030304, 0.03125, 0.03225806451612903, 0.03333333333333333, 0.034482758620689655, 0.03571428571428571, 0.037037037037037035, 0.038461538461538464, 0.04, 0.041666666666666664, 0.043478260869565216, 0.045454545454545456, 0.047619047619047616, 0.05, 0.05128205128205128, 0.05263157894736842, 0.05405405405405406, 0.05555555555555555, 0.05714285714285714, 0.058823529411764705, 0.06060606060606061, 0.0625, 0.06451612903225806, 0.06666666666666667, 0.06896551724137931, 0.06976744186046512, 0.07142857142857142, 0.07317073170731707, 0.07407407407407407, 0.075, 0.07692307692307693, 0.07894736842105263, 0.08, 0.08108108108108109, 0.08333333333333333, 0.08571428571428572, 0.08695652173913043, 0.08823529411764706, 0.09090909090909091, 0.09375, 0.09523809523809523, 0.0967741935483871, 0.0975609756097561, 0.1, 0.10256410256410256, 0.10344827586206896, 0.10526315789473684, 0.10714285714285714, 0.10810810810810811, 0.1111111111111111, 0.11428571428571428, 0.11538461538461539, 0.11764705882352941, 0.12, 0.12121212121212122, 0.12195121951219512, 0.125, 0.1282051282051282, 0.12903225806451613, 0.13043478260869565, 0.13157894736842105, 0.13333333333333333, 0.13513513513513514, 0.13636363636363635, 0.13793103448275862, 0.1388888888888889, 0.14285714285714285, 0.14705882352941177, 0.14814814814814814, 0.15, 0.15151515151515152, 0.15384615384615385, 0.15625, 0.15789473684210525, 0.16, 0.16129032258064516, 0.16216216216216217, 0.16666666666666666, 0.17142857142857143, 0.1724137931034483, 0.17391304347826086, 0.175, 0.17647058823529413, 0.17857142857142858, 0.1794871794871795, 0.18181818181818182, 0.18421052631578946, 0.18518518518518517, 0.1875, 0.19047619047619047, 0.19230769230769232, 0.1935483870967742, 0.19444444444444445, 0.2, 0.20512820512820512, 0.20588235294117646, 0.20689655172413793, 0.20833333333333334, 0.21052631578947367, 0.21212121212121213, 0.21428571428571427, 0.21739130434782608, 0.21875, 0.2222222222222222, 0.225, 0.22580645161290322, 0.22727272727272727, 0.22857142857142856, 0.23076923076923078, 0.23333333333333334, 0.23529411764705882, 0.23809523809523808, 0.24, 0.2413793103448276, 0.24242424242424243, 0.25, 0.2564102564102564, 0.2571428571428571, 0.25806451612903225, 0.25925925925925924, 0.2608695652173913, 0.2631578947368421, 0.2647058823529412, 0.26666666666666666, 0.2692307692307692, 0.2702702702702703, 0.2727272727272727, 0.27586206896551724, 0.2777777777777778, 0.28, 0.28125, 0.2857142857142857, 0.2903225806451613, 0.2916666666666667, 0.29411764705882354, 0.2962962962962963, 0.3, 0.30434782608695654, 0.3076923076923077, 0.3103448275862069, 0.3125, 0.3157894736842105, 0.3181818181818182, 0.32, 0.32142857142857145, 0.3225806451612903, 0.3333333333333333, 0.34615384615384615, 0.34782608695652173, 0.35, 0.35294117647058826, 0.35714285714285715, 0.36, 0.36363636363636365, 0.3684210526315789, 0.37037037037037035, 0.375, 0.38095238095238093, 0.38461538461538464, 0.3888888888888889, 0.391304347826087, 0.4, 0.4090909090909091, 0.4117647058823529, 0.4166666666666667, 0.42105263157894735, 0.4230769230769231, 0.42857142857142855, 0.43478260869565216, 0.4375, 0.44, 0.4444444444444444, 0.45, 0.45454545454545453, 0.4583333333333333, 0.46153846153846156, 0.4666666666666667, 0.47058823529411764, 0.47368421052631576, 0.47619047619047616, 0.5, 0.5263157894736842, 0.5294117647058824, 0.5333333333333333, 0.5384615384615384, 0.5454545454545454, 0.55, 0.5555555555555556, 0.5714285714285714, 0.5789473684210527, 0.5833333333333334, 0.5882352941176471, 0.6, 0.6111111111111112, 0.6153846153846154, 0.631578947368421, 0.6470588235294118, 0.6666666666666666, 0.7333333333333333, 0.7647058823529411, 0.7857142857142857, 0.8571428571428571]\n",
      "Quartile bins (start to end):\n",
      "Q1: 0.0 to 0.06666666666666667\n",
      "Q2: 0.06666666666666667 to 0.1111111111111111\n",
      "Q3: 0.1111111111111111 to 0.16\n",
      "Q4: 0.16 to 0.8571428571428571\n",
      "overlap_all\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15]\n",
      "Quartile bins (start to end):\n",
      "Q1: 0.0 to 1.0\n",
      "Q2: 1.0 to 2.0\n",
      "Q3: 2.0 to 3.0\n",
      "Q4: 3.0 to 15.0\n",
      "overlap_obs1\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Quartile bins (start to end):\n",
      "Q1: 0.0 to 1.0\n",
      "Q2: 1.0 to 2.0\n",
      "Q3: 2.0 to 12.0\n",
      "overlap_obs2\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "Quartile bins (start to end):\n",
      "Q1: 0.0 to 1.0\n",
      "Q2: 1.0 to 2.0\n",
      "Q3: 2.0 to 12.0\n",
      "sentiment_obs\n",
      "[-2, -1, 0, 1, 2, 3]\n",
      "Quartile bins (start to end):\n",
      "Q1: -2.0 to 0.0\n",
      "Q2: 0.0 to 3.0\n",
      "sentiment_obs1\n",
      "[-2, -1, 0, 1, 2]\n",
      "Quartile bins (start to end):\n",
      "Q1: -2.0 to 0.0\n",
      "Q2: 0.0 to 2.0\n",
      "sentiment_obs2\n",
      "[-1, 0, 1, 2]\n",
      "Quartile bins (start to end):\n",
      "Q1: -1.0 to 0.0\n",
      "Q2: 0.0 to 2.0\n",
      "sentiment_hyp\n",
      "[-2, -1, 0, 1, 2, 3]\n",
      "Quartile bins (start to end):\n",
      "Q1: -2.0 to 0.0\n",
      "Q2: 0.0 to 3.0\n"
     ]
    }
   ],
   "source": [
    "d={}\n",
    "for i in ['jaccard_obs1_hyp','jaccard_obs2_hyp','jaccard_obs_hyp','overlap_all', 'overlap_obs1', 'overlap_obs2',\n",
    "       'sentiment_obs', 'sentiment_obs1', 'sentiment_obs2', 'sentiment_hyp']:\n",
    "      print(i)\n",
    "      print(sorted(df_features[i].unique()))\n",
    "      x, bins = pd.qcut(df_features[i], 4, labels=False, retbins=True,duplicates='drop')\n",
    "      d[i]=bins\n",
    "      print(\"Quartile bins (start to end):\")\n",
    "      for i in range(len(bins) - 1):\n",
    "         print(f\"Q{i+1}: {bins[i]} to {bins[i+1]}\")\n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'jaccard_obs1_hyp': array([0.        , 0.0625    , 0.1       , 0.16666667, 1.        ]),\n",
       " 'jaccard_obs2_hyp': array([0.        , 0.04761905, 0.07692308, 0.13333333, 0.83333333]),\n",
       " 'jaccard_obs_hyp': array([0.        , 0.06666667, 0.11111111, 0.16      , 0.85714286]),\n",
       " 'overlap_all': array([ 0.,  1.,  2.,  3., 15.]),\n",
       " 'overlap_obs1': array([ 0.,  1.,  2., 12.]),\n",
       " 'overlap_obs2': array([ 0.,  1.,  2., 12.]),\n",
       " 'sentiment_obs': array([-2.,  0.,  3.]),\n",
       " 'sentiment_obs1': array([-2.,  0.,  2.]),\n",
       " 'sentiment_obs2': array([-1.,  0.,  2.]),\n",
       " 'sentiment_hyp': array([-2.,  0.,  3.])}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow_vectors_optimized2(df, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    \n",
    "    X = [[]for i in range(len(df))]\n",
    "    # Each text segment contributes with a different offset\n",
    "    segments = ['cleaned_text', 'hyp', 'obs1', 'obs2']\n",
    "    \n",
    "    for segment_num, segment in enumerate(segments):\n",
    "        offset = segment_num * vocab_size\n",
    "        \n",
    "        for i, text in enumerate(df[segment]):\n",
    "            \n",
    "            words = text.lower().split()\n",
    "            counts = Counter(w for w in words if w in vocab)\n",
    "            for word, count in counts.items():\n",
    "                idx = word_to_index[word] + offset\n",
    "                X[i].append((idx,count))\n",
    "    for row,i in enumerate(df.itter_rows()):\n",
    "        features=extract_features(row)\n",
    "        for value,j in enumerate(features):\n",
    "            if value>0:\n",
    "                X[i].append((4*vocab_size+j,value))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 [0.05555555555555555, 1, 0, 0, 0, 0.125, 0, 0, 1, 0, 0.08, 0, 1, 0, 0, 2, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "features=extract_features(train_df.iloc[0])\n",
    "print(len(features),features)\n",
    "temp=[]\n",
    "for j,value in enumerate(features):\n",
    "    if value>0:\n",
    "        temp.append((4*vocab_size+j,value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(106812, 0.05555555555555555),\n",
       " (106813, 1),\n",
       " (106817, 0.125),\n",
       " (106820, 1),\n",
       " (106822, 0.08),\n",
       " (106824, 1),\n",
       " (106827, 2),\n",
       " (106829, 1),\n",
       " (106834, 1),\n",
       " (106835, 1),\n",
       " (106841, 2),\n",
       " (106843, 1),\n",
       " (106850, 1),\n",
       " (106854, 1),\n",
       " (106858, 1),\n",
       " (106862, 1)]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def generate_bow_vectors_optimized(df, vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    word_index = word_to_index  # local reference\n",
    "    segments = ['cleaned_text', 'hyp', 'obs1', 'obs2']\n",
    "    X = [[] for _ in range(len(df))]\n",
    "\n",
    "    for i, row in enumerate(df.itertuples(index=False)):\n",
    "        for segment_num, segment in enumerate(segments):\n",
    "            offset = segment_num * vocab_size\n",
    "            words = getattr(row, segment).split()\n",
    "            counts = Counter(w for w in words if w in vocab)\n",
    "            for word, count in counts.items():\n",
    "                X[i].append((word_index[word] + offset, count))\n",
    "\n",
    "        features = extract_features(row._asdict())  # assuming extract_features takes a dict\n",
    "        for j, value in enumerate(features):\n",
    "            if value > 0:\n",
    "                X[i].append((4 * vocab_size + j, value))\n",
    "\n",
    "    return X\n",
    "X=generate_bow_vectors_optimized(train_df,vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in enumerate(X):\n",
    "    for j, (idx, val) in enumerate(row):\n",
    "        if not isinstance(idx, int):\n",
    "            print(f\"Non-int index found at row {i}, tuple {j}: {idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106868"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0][-1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106868"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size*4+56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5329634464751958"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m=load_model('model3.pkl')\n",
    "m.max_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from generate_sparse_vector import sparse_vector\n",
    "from tqdm import tqdm\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "def step_fn(x):\n",
    "    return np.where(x > 0, 1, -1)\n",
    "\n",
    "class Perceptron_op:\n",
    "    def __init__(self, l_r=0.10, n_iters=1000,decay_rate=0.99,best=False):\n",
    "        self.lr = l_r\n",
    "        self.best=best\n",
    "        self.n_iters = n_iters\n",
    "        self.decay_rate = decay_rate  # decay factor for exponential decay\n",
    "        self.activation_fn = step_fn\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.best_weights=None\n",
    "        self.best_bias=None\n",
    "        self.max_acc=0\n",
    "\n",
    "    def decay_lr(self, iteration):\n",
    "        \"\"\"Apply exponential learning rate decay.\"\"\"\n",
    "        return self.lr * (self.decay_rate ** iteration)\n",
    "    \n",
    "    def partial_fit(self, X_train, Y, X_dev,Y_dev):\n",
    "        '''\n",
    "        Inputs: \n",
    "          For training:\n",
    "            X_train         list of features represented in the form of a sparse vector dictionary for each row of training data\n",
    "            Y               list of actual predictions for the training data\n",
    "          To check change in accuracy and save best weights\n",
    "            X_dev           list of features represented in the form of a sparse vector dictionary for each row of dev data\n",
    "            Y_dev           list of actual predictions for the dev data\n",
    "\n",
    "        Outputs:\n",
    "            Prints accuracy for each iteration\n",
    "            saves latest weights and bias and best weights for model  \n",
    "        '''\n",
    "        n_samples = len(Y)\n",
    "        n_features = len(vocab)*4 + 56 #extra fetures from text and bins\n",
    "        self.weights = np.random.rand(n_features) if self.weights is None else self.weights\n",
    "        self.bias = 0 if self.bias is None else self.bias\n",
    "        pbar=tqdm(total=self.n_iters,desc=\"Training iterations\")\n",
    "        bar=0#update progress bar\n",
    "        for i in range(self.n_iters):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            lr = self.decay_lr(i)\n",
    "\n",
    "            for idx in indices:\n",
    "                x_list = X_train[idx]\n",
    "                y_true = Y[idx]\n",
    "\n",
    "                if not x_list: continue\n",
    "\n",
    "                indices_arr, values_arr = zip(*x_list)\n",
    "                indices_arr = list(indices_arr)\n",
    "                values_arr = np.array(values_arr)\n",
    "\n",
    "                score = np.dot(self.weights[indices_arr], values_arr)\n",
    "                y_pred = self.activation_fn(score + self.bias)\n",
    "\n",
    "                if y_pred != y_true:\n",
    "                    update = lr * (y_true - y_pred)\n",
    "                    for index, value in x_list:\n",
    "                        self.weights[index] += update * value\n",
    "                    self.bias += update\n",
    "\n",
    "            y_pred=[self.pred(x)for x in X_dev]\n",
    "            accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "            bar+=1\n",
    "            if (i + 1) % 100 == 0:\n",
    "                pbar.update(bar)\n",
    "                bar=0\n",
    "                print(f'Iter {i + 1}: Accuracy = {accuracy:.5f}, Max Accuracy = {self.max_acc:.5f}')\n",
    "                \n",
    "            if accuracy>self.max_acc:\n",
    "                pbar.update(bar)\n",
    "                bar=0\n",
    "                print (f'iter {i+1} of {self.n_iters} : Accuracy = {accuracy}')\n",
    "                self.best_weights=self.weights.copy()\n",
    "                self.best_bias=self.bias\n",
    "                self.max_acc=accuracy\n",
    "            else:\n",
    "                self.weights=self.best_weights\n",
    "                self.bias=self.best_bias\n",
    "\n",
    "        pbar.close()\n",
    "    def pred(self, X,best=False):\n",
    "        weights=self.weights\n",
    "        bias=self.bias\n",
    "        if best:\n",
    "            weights=self.best_weights\n",
    "            bias=self.best_bias\n",
    "        op=0\n",
    "        for index, value in X:\n",
    "                op+= weights[index]*value\n",
    "        y_pred = self.activation_fn(op+bias)\n",
    "        return y_pred\n",
    "   \n",
    "            \n",
    "# Pickle to store and load model\n",
    "def store_model(model, filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    return\n",
    "def load_model(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        model=pickle.load(f)\n",
    "    return model\n",
    "    \n",
    "# if __name__==\"__main__\":\n",
    "\n",
    "    import pandas as pd\n",
    "    import json\n",
    "\n",
    "    from evaluation import evaluate_predictions\n",
    "    from read_data import read_data\n",
    "    from preprocess import remove_punctuations,create_vocabulary\n",
    "    \n",
    "    batch_size=2000\n",
    "\n",
    "    train_data='train.json'\n",
    "    train_labels='train_labels.lst'\n",
    "\n",
    "    test_data=\"test.json\"\n",
    "    test_labels='test_labels.lst'\n",
    "    dev_data='dev.jsonl'\n",
    "    dev_labels='dev-labels.lst'\n",
    "    stopwords=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','a','an','the']\n",
    "    punc=''.join(['.',',',\"'\",\"'\",\":\",\"?\",'!','@','/','&'])\n",
    "    #read train data\n",
    "    train_df=read_data(train_data,train_labels)\n",
    "    #print(type(train_df))\n",
    "    vocab=create_vocabulary(train_df,stopwords,punc)\n",
    "    #print(vocab)\n",
    "    vocab_size=len(vocab)\n",
    "    X_gen = sparse_vector(train_df, vocab)\n",
    "    #print(X_gen[0]==X_gen[1002])\n",
    "    Y = train_df['Y'].values \n",
    "    dev_df=read_data(dev_data,dev_labels)\n",
    "    X_dev=sparse_vector(dev_df,vocab)\n",
    "    Y_dev=dev_df['Y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from generate_sparse_vector import sparse_vector\n",
    "from tqdm import tqdm\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def step_fn(x):\n",
    "    return np.where(x > 0, 1, -1)\n",
    "\n",
    "class Perceptron_curr:\n",
    "    def __init__(self, l_r=0.10, n_iters=1000,decay_rate=0.99,best=False):\n",
    "        self.lr = l_r\n",
    "        self.best=best\n",
    "        self.n_iters = n_iters\n",
    "        self.decay_rate = decay_rate  # decay factor for exponential decay\n",
    "        self.activation_fn = step_fn\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.best_weights=None\n",
    "        self.best_bias=None\n",
    "        self.max_acc=0\n",
    "\n",
    "    def decay_lr(self, iteration):\n",
    "        \"\"\"Apply exponential learning rate decay.\"\"\"\n",
    "        return self.lr * (self.decay_rate ** iteration)\n",
    "    \n",
    "    def partial_fit(self, X_train, Y, X_dev,Y_dev):\n",
    "        '''\n",
    "        Inputs: \n",
    "          For training:\n",
    "            X_train         list of features represented in the form of a sparse vector dictionary for each row of training data\n",
    "            Y               list of actual predictions for the training data\n",
    "          To check change in accuracy and save best weights\n",
    "            X_dev           list of features represented in the form of a sparse vector dictionary for each row of dev data\n",
    "            Y_dev           list of actual predictions for the dev data\n",
    "\n",
    "        Outputs:\n",
    "            Prints accuracy for each iteration\n",
    "            saves latest weights and bias and best weights for model  \n",
    "        '''\n",
    "        n_samples = len(Y)\n",
    "        n_features = len(vocab)*4 + 56 #extra fetures from text and bins\n",
    "        self.weights = np.random.rand(n_features) if self.weights is None else self.weights\n",
    "        self.bias = 0 if self.bias is None else self.bias\n",
    "        pbar=tqdm(total=self.n_iters,desc=\"Training iterations\")\n",
    "        bar=0#update progress bar\n",
    "        for i in range(self.n_iters):\n",
    "            \n",
    "            # Shuffle data at the start of each epoch to improve generalization\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_train = [X_train[idx] for idx in indices]\n",
    "            Y = [Y[idx] for idx in indices]\n",
    "\n",
    "            #set current l_r\n",
    "            lr=self.decay_lr(i)\n",
    "            for x_list, y_true in zip(X_train, Y):\n",
    "                score=0 # initialize value to 0 for each row of training data\n",
    "                for index, value in x_list:\n",
    "                    score+= self.weights[index]*value\n",
    "                y_pred = self.activation_fn(score+self.bias)\n",
    "                if y_pred != y_true:\n",
    "                    update = lr * (y_true - y_pred)\n",
    "                    for index, value in x_list:\n",
    "                        self.weights[index]+= update * value \n",
    "                    self.bias += update\n",
    "\n",
    "            y_pred=[self.pred(x,self.best) for x in X_dev]\n",
    "            accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "            bar+=1\n",
    "            if (i + 1) % 100 == 0:\n",
    "                pbar.update(bar)\n",
    "                bar=0\n",
    "                print(f'Iter {i + 1}: Accuracy = {accuracy:.5f}, Max Accuracy = {self.max_acc:.5f}')\n",
    "                \n",
    "            if accuracy>self.max_acc:\n",
    "                pbar.update(bar)\n",
    "                bar=0\n",
    "                print (f'iter {i+1} of {self.n_iters} : Accuracy = {accuracy}')\n",
    "                self.best_weights=self.weights.copy()\n",
    "                self.best_bias=self.bias\n",
    "                self.max_acc=accuracy\n",
    "\n",
    "        pbar.close()\n",
    "    def pred(self, X,best=False):\n",
    "        weights=self.weights\n",
    "        bias=self.bias\n",
    "        if best:\n",
    "            weights=self.best_weights\n",
    "            bias=self.best_bias\n",
    "        op=0\n",
    "        for index, value in X:\n",
    "                op+= weights[index]*value\n",
    "        y_pred = self.activation_fn(op+bias)\n",
    "        return y_pred\n",
    "   \n",
    "            \n",
    "# Pickle to store and load model\n",
    "def store_model(model, filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    return\n",
    "def load_model(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        model=pickle.load(f)\n",
    "    return model\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 10 : Accuracy = 0.5137075718015666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3 of 10 : Accuracy = 0.5156657963446475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4 of 10 : Accuracy = 0.5182767624020888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7 of 10 : Accuracy = 0.5215404699738904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:  70%|███████   | 7/10 [01:07<00:29,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 1s\n",
      "Wall time: 1min 7s\n",
      "0.5199086161879896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model = Perceptron_op(l_r=0.9,n_iters=10)\n",
    "\n",
    "\n",
    "# model=load_model('model4.pkl')\n",
    "# model.n_iters=100\n",
    "# model.weights=model.best_weights.copy()\n",
    "print(\"Training Model\")\n",
    "%time model.partial_fit(X_gen, Y, X_dev,Y_dev)\n",
    "# model_fname=\"model_op.pkl\"\n",
    "# print(f\"Storing Model {model_fname}\")\n",
    "# store_model(model,model_fname)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # testing model performance using dev data\n",
    "\n",
    "y_pred=[model.pred(x) for x in X_dev]\n",
    "accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "print(accuracy)\n",
    "# Accuracy=0.5094647519582245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 10 : Accuracy = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2 of 10 : Accuracy = 0.5019582245430809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3 of 10 : Accuracy = 0.5032637075718016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4 of 10 : Accuracy = 0.5045691906005222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5 of 10 : Accuracy = 0.5071801566579635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 6 of 10 : Accuracy = 0.5075065274151436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 7 of 10 : Accuracy = 0.5097911227154047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:  70%|███████   | 7/10 [01:07<00:29,  9.69s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 10 : Accuracy = 0.5101174934725848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 4 of 10 : Accuracy = 0.5172976501305483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:  40%|████      | 4/10 [01:06<01:40, 16.72s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 10 : Accuracy = 0.5094647519582245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 3 of 10 : Accuracy = 0.5189295039164491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 5 of 10 : Accuracy = 0.5195822454308094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8 of 10 : Accuracy = 0.5215404699738904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:  80%|████████  | 8/10 [01:06<00:16,  8.35s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 10 : Accuracy = 0.508485639686684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2 of 10 : Accuracy = 0.5176240208877284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8 of 10 : Accuracy = 0.5225195822454308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:  80%|████████  | 8/10 [01:06<00:16,  8.33s/it]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1 of 10 : Accuracy = 0.514686684073107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 2 of 10 : Accuracy = 0.5248041775456919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 8 of 10 : Accuracy = 0.5261096605744126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:  80%|████████  | 8/10 [01:06<00:16,  8.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best learning rate is: 1.0 with accuracy: 0.5221932114882507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "best_lr = None\n",
    "best_accuracy = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    model = Perceptron_op(l_r=lr, n_iters=10, decay_rate=0.99)\n",
    "    model.partial_fit(X_gen, Y, X_dev, Y_dev)\n",
    "    \n",
    "    # Check performance on the dev set\n",
    "    y_pred = [model.pred(x, best=True) for x in X_dev]\n",
    "    accuracy = evaluate_predictions(y_pred, Y_dev)\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_lr = lr\n",
    "\n",
    "print(f\"The best learning rate is: {best_lr} with accuracy: {best_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         16151349 function calls in 101.996 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.000    0.000  101.996  101.996 2989787380.py:3(train)\n",
      "  4048396   10.852    0.000   11.182    0.000 3799436382.py:10(step_fn)\n",
      "       15    0.000    0.000    0.000    0.000 3799436382.py:26(decay_lr)\n",
      "        1   76.899   76.899  101.996  101.996 3799436382.py:30(partial_fit)\n",
      "       14    0.014    0.001    0.660    0.047 3799436382.py:73(<listcomp>)\n",
      "    42896    0.539    0.000    0.646    0.000 3799436382.py:93(pred)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1053(_handle_fromlist)\n",
      "        1    0.000    0.000  101.996  101.996 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.000    0.000 __init__.py:48(create_string_buffer)\n",
      "       14    0.000    0.000    0.001    0.000 _methods.py:101(_mean)\n",
      "       14    0.000    0.000    0.000    0.000 _methods.py:67(_count_reduce_items)\n",
      "        1    0.000    0.000    0.000    0.000 _monitor.py:94(report)\n",
      "        1    0.000    0.000    0.000    0.000 _weakrefset.py:17(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _weakrefset.py:21(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 _weakrefset.py:27(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 _weakrefset.py:53(_commit_removals)\n",
      "        3    0.000    0.000    0.000    0.000 _weakrefset.py:63(__iter__)\n",
      "        1    0.000    0.000    0.000    0.000 _weakrefset.py:86(add)\n",
      "       14    0.000    0.000    0.003    0.000 evaluation.py:19(evaluate_predictions)\n",
      "       14    0.000    0.000    0.000    0.000 fromnumeric.py:3380(_mean_dispatcher)\n",
      "       14    0.000    0.000    0.001    0.000 fromnumeric.py:3385(mean)\n",
      "        1    0.000    0.000    0.000    0.000 functools.py:393(__get__)\n",
      "        7    0.000    0.000    0.000    0.000 iostream.py:138(_event_pipe)\n",
      "        7    0.000    0.000    0.001    0.000 iostream.py:259(schedule)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:505(parent_header)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:550(_is_master_process)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:577(_schedule_flush)\n",
      "        3    0.000    0.000    0.001    0.000 iostream.py:592(flush)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:655(write)\n",
      "  4048396    0.330    0.000    0.330    0.000 multiarray.py:346(where)\n",
      "  4005501    0.395    0.000    0.395    0.000 multiarray.py:741(dot)\n",
      "        7    0.001    0.000    0.001    0.000 socket.py:621(send)\n",
      "        3    0.000    0.000    0.000    0.000 std.py:102(acquire)\n",
      "        3    0.000    0.000    0.000    0.000 std.py:106(release)\n",
      "        2    0.000    0.000    0.000    0.000 std.py:110(__enter__)\n",
      "        2    0.000    0.000    0.000    0.000 std.py:113(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:1150(__str__)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:1157(__hash__)\n",
      "        1    0.000    0.000    0.001    0.001 std.py:1325(refresh)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:1446(format_dict)\n",
      "        1    0.000    0.000    0.001    0.001 std.py:1464(display)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:153(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:163(colour)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:167(colour)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:186(__format__)\n",
      "        3    0.000    0.000    0.000    0.000 std.py:226(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:231(__call__)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:400(format_interval)\n",
      "        1    0.000    0.000    0.001    0.001 std.py:438(status_printer)\n",
      "        1    0.000    0.000    0.001    0.001 std.py:451(fp_write)\n",
      "        1    0.000    0.000    0.001    0.001 std.py:457(print_status)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:464(format_meter)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:663(__new__)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:679(_get_free_pos)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:682(<setcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 std.py:760(get_lock)\n",
      "        1    0.000    0.000    0.002    0.002 std.py:952(__init__)\n",
      "       10    0.000    0.000    0.000    0.000 threading.py:1102(_wait_for_tstate_lock)\n",
      "        6    0.000    0.000    0.000    0.000 threading.py:1145(ident)\n",
      "       10    0.000    0.000    0.000    0.000 threading.py:1169(is_alive)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:1430(current_thread)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:236(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:264(__enter__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:267(__exit__)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:273(_release_save)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:276(_acquire_restore)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:279(_is_owned)\n",
      "        3    0.000    0.000    0.001    0.000 threading.py:288(wait)\n",
      "        3    0.000    0.000    0.000    0.000 threading.py:545(__init__)\n",
      "       11    0.000    0.000    0.000    0.000 threading.py:553(is_set)\n",
      "        3    0.000    0.000    0.001    0.000 threading.py:589(wait)\n",
      "        1    0.000    0.000    0.000    0.000 tz.py:74(utcoffset)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:108(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:112(__format__)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:139(__getattr__)\n",
      "        3    0.000    0.000    0.000    0.000 utils.py:152(wrapper_setattr)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:156(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:187(disable_on_exception)\n",
      "        2    0.000    0.000    0.001    0.000 utils.py:194(inner)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:213(__init__)\n",
      "        4    0.000    0.000    0.000    0.000 utils.py:222(__eq__)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:252(_is_utf)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:266(_supports_unicode)\n",
      "        2    0.000    0.000    0.000    0.000 utils.py:273(_is_ascii)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:282(_screen_shape_wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:297(_screen_shape_windows)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:374(_text_width)\n",
      "       61    0.000    0.000    0.000    0.000 utils.py:375(<genexpr>)\n",
      "        1    0.000    0.000    0.000    0.000 utils.py:378(disp_len)\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x00007FF85A1C5EF0}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _weakref.proxy}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.abs}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
      "        1    0.000    0.000  101.996  101.996 {built-in method builtins.exec}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method builtins.hasattr}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method builtins.isinstance}\n",
      "       14    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "       21    0.000    0.000    0.000    0.000 {built-in method builtins.ord}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method fromtimestamp}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "  4005529   12.885    0.000   12.885    0.000 {built-in method numpy.array}\n",
      "       14    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
      "       14    0.000    0.000    0.000    0.000 {built-in method numpy.core._multiarray_umath.normalize_axis_index}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method sys.audit}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "       60    0.000    0.000    0.000    0.000 {built-in method unicodedata.east_asian_width}\n",
      "        3    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_multiprocessing.SemLock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'acquire' of '_thread.RLock' objects}\n",
      "       22    0.001    0.000    0.001    0.000 {method 'acquire' of '_thread.lock' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'add' of 'set' objects}\n",
      "       10    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'difference' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'get' of '_contextvars.ContextVar' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "       15    0.078    0.005    0.078    0.005 {method 'permutation' of 'numpy.random.mtrand.RandomState' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "       14    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'release' of '_multiprocessing.SemLock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'release' of '_thread.RLock' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'release' of '_thread.lock' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'remove' of 'set' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'sub' of 're.Pattern' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[0;32m      4\u001b[0m     model\u001b[38;5;241m.\u001b[39mpartial_fit(X_gen, Y, X_dev, Y_dev)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mcProfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain()\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\.conda\\envs\\CUDA\\lib\\cProfile.py:17\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(statement, filename, sort)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(statement, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_pyprofile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Utils\u001b[49m\u001b[43m(\u001b[49m\u001b[43mProfile\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\.conda\\envs\\CUDA\\lib\\profile.py:54\u001b[0m, in \u001b[0;36m_Utils.run\u001b[1;34m(self, statement, filename, sort)\u001b[0m\n\u001b[0;32m     52\u001b[0m prof \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprofiler()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mprof\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\asus\\.conda\\envs\\CUDA\\lib\\cProfile.py:96\u001b[0m, in \u001b[0;36mProfile.run\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01m__main__\u001b[39;00m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mdict\u001b[39m \u001b[38;5;241m=\u001b[39m __main__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\n\u001b[1;32m---> 96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunctx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\asus\\.conda\\envs\\CUDA\\lib\\cProfile.py:101\u001b[0m, in \u001b[0;36mProfile.runctx\u001b[1;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable()\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 101\u001b[0m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable()\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m():\n\u001b[1;32m----> 4\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_dev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_dev\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 64\u001b[0m, in \u001b[0;36mPerceptron_op.partial_fit\u001b[1;34m(self, X_train, Y, X_dev, Y_dev)\u001b[0m\n\u001b[0;32m     61\u001b[0m indices_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(indices_arr)\n\u001b[0;32m     62\u001b[0m values_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(values_arr)\n\u001b[1;32m---> 64\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices_arr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_fn(score \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_pred \u001b[38;5;241m!=\u001b[39m y_true:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "\n",
    "def train():\n",
    "    model.partial_fit(X_gen, Y, X_dev, Y_dev)\n",
    "\n",
    "cProfile.run('train()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Perceptron_curr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model2 \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptron_curr\u001b[49m(l_r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,n_iters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# model=load_model('model4.pkl')\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# model.n_iters=100\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# model.weights=model.best_weights.copy()\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Perceptron_curr' is not defined"
     ]
    }
   ],
   "source": [
    "# training\n",
    "model2 = Perceptron_curr(l_r=0.3,n_iters=100)\n",
    "\n",
    "\n",
    "# model=load_model('model4.pkl')\n",
    "# model.n_iters=100\n",
    "# model.weights=model.best_weights.copy()\n",
    "print(\"Training Model\")\n",
    "%time model2.partial_fit(X_gen, Y, X_dev,Y_dev)\n",
    "model_fname=\"model4.pkl\"\n",
    "# print(f\"Storing Model {model_fname}\")\n",
    "# store_model(model,model_fname)\n",
    "\n",
    "\n",
    "\n",
    "# testing model performance using dev data\n",
    "\n",
    "y_pred=[model2.pred(x) for x in X_dev]\n",
    "accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "\n",
    "# Accuracy=0.5094647519582245\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "def pred_fn(model,x):\n",
    "    return model.pred(x)\n",
    "with multiprocessing.Pool() as pool:\n",
    "    pred_fn = partial(self.pred, best=self.best)\n",
    "    y_pred = pool.map(pred_fn, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time [model2.pred(x)for x in X_dev]\n",
    "with multiprocessing.Pool() as pool:\n",
    "    pred_fn = partial(self.pred, best=self.best)\n",
    "    y_pred = pool.map(pred_fn, X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training iterations:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "%time model_mul.partial_fit(X_gen, Y, X_dev,Y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_features import extract_features\n",
    "def sparse_bow(df,vocab):\n",
    "    vocab_size = len(vocab)\n",
    "    word_index = {word: i for i, word in enumerate(sorted(vocab))}  # local reference\n",
    "    segments = ['cleaned_text', 'hyp', 'obs1', 'obs2']\n",
    "    X = [[] for _ in range(len(df))]\n",
    "\n",
    "    for i, row in enumerate(df.itertuples(index=False)):\n",
    "        for segment_num, segment in enumerate(segments):\n",
    "            offset = segment_num * vocab_size\n",
    "            words = getattr(row, segment).split()\n",
    "            counts = Counter(w for w in words if w in vocab)\n",
    "            for word, count in counts.items():\n",
    "                X[i].append((word_index[word] + offset, count))\n",
    "    return X\n",
    "def sparse_feat(df,vocab):\n",
    "    X = [[] for _ in range(len(df))]\n",
    "    for i, row in enumerate(df.itertuples(index=False)):\n",
    "        features = extract_features(row._asdict())  # extract_features takes a dict as input\n",
    "        for j, value in enumerate(features):\n",
    "            if value > 0:\n",
    "                X[i].append((j, value))\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONLY BOW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only BOW\n",
    "import pandas as pd\n",
    "import json\n",
    "from generate_sparse_vector import sparse_vector\n",
    "from evaluation import evaluate_predictions\n",
    "from read_data import read_data\n",
    "from preprocess import remove_punctuations,create_vocabulary\n",
    "\n",
    "batch_size=2000\n",
    "\n",
    "train_data='train.json'\n",
    "train_labels='train_labels.lst'\n",
    "\n",
    "test_data=\"test.json\"\n",
    "test_labels='test_labels.lst'\n",
    "dev_data='dev.jsonl'\n",
    "dev_labels='dev-labels.lst'\n",
    "stopwords=['i','me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their','theirs','themselves','a','an','the']\n",
    "punc=''.join(['.',',',\"'\",\"'\",\":\",\"?\",'!','@','/','&'])\n",
    "#read train data\n",
    "train_df=read_data(train_data,train_labels)\n",
    "#print(type(train_df))\n",
    "vocab=create_vocabulary(train_df,stopwords,punc)\n",
    "#print(vocab)\n",
    "vocab_size=len(vocab)\n",
    "X_gen = sparse_bow(train_df, vocab)\n",
    "#print(X_gen[0]==X_gen[1002])\n",
    "Y = train_df['Y'].values \n",
    "dev_df=read_data(dev_data,dev_labels)\n",
    "X_dev=sparse_bow(dev_df,vocab)\n",
    "Y_dev=dev_df['Y'].values\n",
    "test_df=read_data(test_data,test_labels)\n",
    "X_test=sparse_bow(test_df,vocab)\n",
    "Y_test=X_test['Y'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from generate_sparse_vector import sparse_vector\n",
    "from tqdm import tqdm\n",
    "import _pickle as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def step_fn(x):\n",
    "    return np.where(x > 0, 1, -1)\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, l_r=0.10, n_iters=1000,decay_rate=0.99,best=False):\n",
    "        self.lr = l_r\n",
    "        self.best=best\n",
    "        self.n_iters = n_iters\n",
    "        self.decay_rate = decay_rate  # decay factor for exponential decay\n",
    "        self.activation_fn = step_fn\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.best_weights=None\n",
    "        self.best_bias=None\n",
    "        self.max_acc=0\n",
    "\n",
    "    def decay_lr(self, iteration):\n",
    "        \"\"\"Apply exponential learning rate decay.\"\"\"\n",
    "        return self.lr * (self.decay_rate ** iteration)\n",
    "    \n",
    "    def partial_fit(self, X_train, Y, X_dev,Y_dev):\n",
    "        '''\n",
    "        Inputs: \n",
    "          For training:\n",
    "            X_train         list of features represented in the form of a sparse vector dictionary for each row of training data\n",
    "            Y               list of actual predictions for the training data\n",
    "          To check change in accuracy and save best weights\n",
    "            X_dev           list of features represented in the form of a sparse vector dictionary for each row of dev data\n",
    "            Y_dev           list of actual predictions for the dev data\n",
    "\n",
    "        Outputs:\n",
    "            Prints accuracy for each iteration\n",
    "            saves latest weights and bias and best weights for model  \n",
    "        '''\n",
    "        n_samples = len(Y)\n",
    "        n_features = len(vocab)*4 #extra fetures from text and bins\n",
    "        self.weights = np.random.rand(n_features) if self.weights is None else self.weights\n",
    "        self.bias = 0 if self.bias is None else self.bias\n",
    "        pbar=tqdm(total=self.n_iters,desc=\"Training iterations\")\n",
    "        bar=0#update progress bar\n",
    "        for i in range(self.n_iters):\n",
    "            \n",
    "            # Shuffle data at the start of each epoch to improve generalization\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_train = [X_train[idx] for idx in indices]\n",
    "            Y = [Y[idx] for idx in indices]\n",
    "\n",
    "            #set current l_r\n",
    "            lr=self.decay_lr(i)\n",
    "            for x_list, y_true in zip(X_train, Y):\n",
    "                score=0 # initialize value to 0 for each row of training data\n",
    "                for index, value in x_list:\n",
    "                    score+= self.weights[index]*value\n",
    "                y_pred = self.activation_fn(score+self.bias)\n",
    "                if y_pred != y_true:\n",
    "                    update = lr * (y_true - y_pred)\n",
    "                    for index, value in x_list:\n",
    "                        self.weights[index]+= update * value \n",
    "                    self.bias += update\n",
    "\n",
    "            y_pred=[self.pred(x,self.best) for x in X_dev]\n",
    "            accuracy=evaluate_predictions(y_pred,Y_dev)\n",
    "            bar+=1\n",
    "            if (i + 1) % 100 == 0:\n",
    "                pbar.update(bar)\n",
    "                bar=0\n",
    "                print(f'Iter {i + 1}: Accuracy = {accuracy:.5f}, Max Accuracy = {self.max_acc:.5f}')\n",
    "                \n",
    "            if accuracy>self.max_acc:\n",
    "                pbar.update(bar)\n",
    "                bar=0\n",
    "                print (f'iter {i+1} of {self.n_iters} : Accuracy = {accuracy}')\n",
    "                self.best_weights=self.weights.copy()\n",
    "                self.best_bias=self.bias\n",
    "                self.max_acc=accuracy\n",
    "            if self.best:\n",
    "                self.weights=self.best_weights\n",
    "                self.bias=self.best_bias\n",
    "\n",
    "        pbar.close()\n",
    "    def pred(self, X,best=False):\n",
    "        weights=self.weights\n",
    "        bias=self.bias\n",
    "        if best:\n",
    "            weights=self.best_weights\n",
    "            bias=self.best_bias\n",
    "        op=0\n",
    "        for index, value in X:\n",
    "                op+= weights[index]*value\n",
    "        y_pred = self.activation_fn(op+bias)\n",
    "        return y_pred\n",
    "   \n",
    "            \n",
    "# Pickle to store and load model\n",
    "def store_model(model, filename):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    return\n",
    "def load_model(filename):\n",
    "    with open(filename,'rb')as f:\n",
    "        model=pickle.load(f)\n",
    "    return model\n",
    "\n",
    "def store_vocab(vocab,filename='vocab.pkl'):\n",
    "    with open(filename,'wb') as f:\n",
    "        pickle.dump(vocab,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_bow=Perceptron()\n",
    "m_bow.partial_fit(X_train,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from perceptron import Perceptron, step_fn,store_model,load_model\n",
    "\n",
    "f='model3.pkl' \n",
    "model=load_model(f)\n",
    "new_model = []\n",
    "\n",
    "# Example 1: Add a new attribute that wasn't there before\n",
    "if not hasattr(model, 'n_features'):\n",
    "    model.n_features = 106868\n",
    "store_model(model,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
